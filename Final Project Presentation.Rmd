---
title: "960:365 -- Final Project"
author: "Nilay Tripathi"
date: "12/8/2022"
output: beamer_presentation
---


```{r warning=F, message=F}
# Load necessary packages
library(bayesrules)
library(bayesplot)
library(rstanarm)
library(tidyverse)
library(tidybayes)
library(broom.mixed)
library(gridExtra)
library(janitor)
```

```{r echo = FALSE, message = FALSE, warning = FALSE}
# This will be used to allow the code to wrap lines when converting it to a slideshow
knitr::opts_chunk$set(tidy.opts = list(width.cutoff = 60), tidy = TRUE)
```

# Importing and Cleaning Data

## Importing the Dataset
```{r}
# Define column names
column_names = c("Symboling", "Normalized_Losses", "Make", "Fuel_Type", "Aspiration", "Num_Doors", "Body_Style", "Drive_Wheels", "Engine_Location", "Wheel_Base", "Length", "Width", "Height", "Curb_Weight", "Engine_Type", "Num_Cylinders", "Engine_Size", "Fuel_System", "Bore", "Stroke", "Compression", "Horsepower", "Peak_RPM", "City_MPG", "Highway_MPG", "Price")

# Read the data
cars_data = read_csv("imports-85.data", col_names = column_names)

# Take a peek at the data
head(cars_data)
```

## Data Cleaning

The data in its current form is very messy. We will now clean the data
```{r}
# The values with a '?' for normalized losses are unknown. Remove them from the data set
cars_df = cars_data %>% 
  filter(Normalized_Losses != "?")
head(cars_df)
# Compare number of observations left
c(nrow(cars_data), nrow(cars_df))
# Overall summary for the different columns
summary(cars_df)
# See number of different makes
cars_df %>%
  tabyl(Make)
# Make a new column called Average_MPG that is the arithmetic mean of the city and highway MPG
cars_df = cars_df %>%
  mutate(Avg_MPG = (City_MPG + Highway_MPG) / 2)
summary(cars_df$Avg_MPG)
# Some of the numeric values have character values. We should change this 
cars_df = cars_df %>%
  mutate(across(.cols = c(2, 19,20,21,22,23,24,26), as.numeric))
# Summary 
summary(cars_df)
# Extra NA values have been identified in column 20. Remove them 
cars_df = cars_df %>% 
  filter(!is.na(Stroke))
summary(cars_df)
```

# Building a Model

We will build a model for `Normalized_Losses`. We will utilize the predictors `price` along with the categorical predictor `Aspiration`.

## Initial Plotting

```{r}
ggplot(data = cars_df, aes(x = Price, y = Normalized_Losses, color = Aspiration)) + 
  geom_point() + 
  geom_smooth(method = "lm", se = FALSE)

# Summary of the values of the cars by fuel type 
cars_df %>% 
  group_by(Aspiration) %>%
  summarize(avg = mean(Normalized_Losses), 
            n = n())
```

The plot above analyzes the relationship between `Normalized_Losses` and `Price`. The relationship doesn't appear too strong in the model. Additionally, there seems to be a noticeable interaction effect since the slopes of the lines for std Aspiration and turbo Aspiration have opposite signs. 

In light of these observations, we will fit two models: one with an interaction term and one without. We will then compare these two models using cross validation and expected log predictive density (ELPD).

## Fitting the No-Interaction Model
Here we will fit the model without interaction effects. 
```{r cache = T, results=FALSE}
no_interaction = stan_glm(
  Normalized_Losses ~ Price + Aspiration,
  data = cars_df, family = gaussian,
  prior_intercept = normal(120, 5, autoscale = TRUE),
  prior = normal(0, 2.5, autoscale = TRUE),
  prior_aux = exponential(1, autoscale = TRUE),
  chains = 4, iter = 5000*2, seed = 12345
)
```

### Prior Summary

Here is an overview of the priors that were selected initially and how the `stan_glm` function adjusted them. The intercept prior was chosen from the output of the `summarize` function under the graph.
```{r}
prior_summary(no_interaction)
```

### MCMC Diagnostics

Here are the MCMC diagnostics for this model
```{r}
# Trace plots
mcmc_trace(no_interaction, size = 0.1)

# Density overlays
mcmc_dens_overlay(no_interaction)

# Autocorrelation plots
mcmc_acf(no_interaction)

# Effective Sample Size ratio
neff_ratio(no_interaction)

# Rhat 
rhat(no_interaction)
```
All of these diagnostics show promising results of the chain. The trace plots exhibit good random scatter, the density overlays are all very similar, and the autocorrelation in the chains quickly approaches 0. Additionally, the effective sample size ratios are all high and the values of $\hat{R}$ are all less than 1.01, which indicates that the chains have mixed well.

### Numerical Output

Generate the numerical output of the model using the `tidy` function.
```{r}
tidy(no_interaction, effects = c("fixed", "aux"), conf.int = TRUE, conf.level = 0.80)
```

### Interpreting the Coefficients

We will now interpret the various coefficients from the model 

- `(Intercept)`: for a typical std aspiration car that cost \$0, the estimated normalized loss is 106. We should note that this intercept doesn't make sense since very few (if any) cars will cost \$0. So the intercept is just here for mathematical reasons. 
- `Price`: for a unit increase in price, the normalized loss will increase by roughly 0.00147. Since this is a no-interaction model, this is the common slope for both lines and so this increase is the same for both std aspiration cars and turbo aspiration cars.
- `Aspirationturbo`: for two cars that were the same price, the normalized loss for a turbo aspiration car is about -9.30 less than that for a std aspiration car.
- `sigma`: this is the estimated variation in possible normalized losses for cars that have the same price.

The output indicates that `Price` is significant (although barely since the coefficients are still pretty close to 0). The categorical variable `Aspiration` does not appear to be significant. We will now calculate the probabilities of these coefficients

### Assessing the Regression Coefficients

We will find the probability that the regression coefficient of `Price` is greater than 0 in our posterior prediction. 
```{r}
no_interaction_df = as.data.frame(no_interaction, include = FALSE)
head(no_interaction_df)

# Probability that Price coefficient is greater than 0
no_interaction_df %>% 
  mutate(regGreaterThan0 = (Price > 0)) %>%
  tabyl(regGreaterThan0)

# Probability that Aspirationturbo coefficient is greater than 0
no_interaction_df %>%
  mutate(aspirationGreaterThan0 = (Aspirationturbo > 0)) %>%
  tabyl(aspirationGreaterThan0)
```
From the output, we see that the probability of the `Price` being greater than 0 is 0.996, which indicates the `Price` is significant, as indicated by the posterior model. However, in this model, the probability that `Aspirationturbo` is greater than 0 is only 0.117 while the probability that it is negative is 0.883. This indicates that this coefficient is most likely negative under the posterior model. As the earlier `tidy` output suggested, however, this coefficient isn't significant in this model. 

### Simulating the Posterior Model 

We will now simulate 100 possible models from the posterior line.
```{r warning = FALSE}
cars_df %>%
  add_fitted_draws(no_interaction, n = 100) %>%
  ggplot(aes(x = Price, y = Normalized_Losses, color = Aspiration)) + 
  geom_line(aes(y = .value, group = paste(Aspiration, .draw)), alpha = .1) +
  geom_point(data = cars_df)
```

All lines indicate that there is a positive association between price and normalized losses. Interestingly, the slope is also positive for turbo cars while the exploratory plot indicated that the correlation is negative. This reflects the fact that the `Aspiration` predictor is not significant in this model.

### Prediction Using This Model

We will now predict the normalized loss of a car which cost \$15,000. The code and the corresponding graphs are shown below
```{r warning = FALSE}
set.seed(12345)

# Simulate the predictions
price_15000_prediction_nointeract = posterior_predict(
  no_interaction, 
  newdata = data.frame(Price = c(15000, 15000),
                       Aspiration = c("std", "turbo"))
)

# Plot the predictions
mcmc_areas(price_15000_prediction_nointeract) +
  ggplot2::scale_y_discrete(labels = c("std", "turbo")) + 
  xlab("Price")
```
<br>
<br>

From the graph, it seems that the predicted normalized loss of a \$15,000 car is a little over 100 for std cars. The predicted normalized loss of a \$15,000 turbo car is a little less, but still over 100.

## Fitting the Interaction Model

The graph indicates that there could be an interaction between these models. Let's fit an interaction model to see if it is better than the non-interaction one.
```{r cache = T, results = FALSE}
interaction_model = stan_glm(
  Normalized_Losses ~ Aspiration + Price + Aspiration:Price, 
  data = cars_df, family = gaussian, 
  prior_intercept = normal(120, 5, autoscale = TRUE),
  prior = normal(0, 2.5, autoscale = TRUE),
  prior_aux = exponential(1, autoscale = TRUE),
  chains = 4, iter = 5000*2, seed = 12345
)
```

### Prior Summary

Here is the prior model that I initially chose and how the `stan_glm` function adjusted them. The intercept prior is the same as that of the interaction model.
```{r}
prior_summary(interaction_model)
```

### MCMC Diagnostics 
```{r}
# Trace plots
mcmc_trace(interaction_model, size = 0.1)

# Density overlays
mcmc_dens_overlay(interaction_model)

# Autocorrelation plots
mcmc_acf(interaction_model)

# Effective sample size ratio
neff_ratio(interaction_model)

# Rhat value
rhat(interaction_model)
```

The diagnostics show that the chains have mixed well and are a good approximation of the posterior distribution. The trace plots display the desired random scatter, the density overlays are all fairly similar and consistent, and the autocorrelation plots indicate that the autocorrelation in the chains quickly approach 0. Additionally, the effective sample size ratios are all greater than 0.1 and the $\hat{R}$ values are all less than 1.01, which indicates the chains have mixed well.

### Numerical Output

Here is the `tidy` summary for this model.
```{r}
tidy(interaction_model, effects = c("fixed", "aux"), conf.int = TRUE, conf.level = 0.80)
```

Since all of the 80% credible intervals do not contain 0 for any of the coefficients, we have that all these coefficients are significant in this model. 

### Interpretation of Coefficients

Here are the interpretation of the coefficients from the interaction model 

- `(Intercept)`: This is the intercept of the lines for std aspiration cars. The normalized loss for a std aspiration car with price \$0 is 95.8. Similar to the no-interaction model, this intercept doesn't make sense in this context.
- `Aspirationturbo`: This is the difference in intercepts between std aspiration cars and turbo aspiration cars. For two cars with the same price, the difference in intercepts is 43.4. - `Price`: This is the slope of the std aspiration cars. For a unit increase in Price, the normalized loss for a std car will increase by roughly 0.00247. 
- `Aspirationturbo:Price`: This is the difference in slopes between std and turbo aspiration cars. The difference in slopes between turbo and std aspiration cars is -0.00354, so this represents the difference in the rate of change of normalized loss for a unit increase in price.
- `sigma`: This is the variability associated with the observations of normalized losses for a fixed value of Price. 

### Simulating The Posterior Model

We will now simulate 100 draws from this posterior model.
```{r warning = FALSE}
cars_df %>%
  add_fitted_draws(interaction_model, n = 100) %>%
  ggplot(aes(x = Price, y = Normalized_Losses, color = Aspiration)) + 
  geom_line(aes(y = .value, group = paste(Aspiration, .draw)), alpha = .1) +
  geom_point(data = cars_df)
```

The posterior draws for the interaction model highlight the difference in slopes between turbo and std cars. However, the draws have considerable variation, which is reflective of the weak overall relationship between the predictors. 

### Posterior Prediction Using This Model

We will now use this model to predict the normalized loss of a \$15,000 car. The code and corresponding graph are shown below
```{r}
set.seed(12345)

# Simulate the predictions
price_15000_prediction_interaction = posterior_predict(
  interaction_model, 
  newdata = data.frame(Price = c(15000, 15000),
                       Aspiration = c("std", "turbo"))
)

# Plot the predictions
mcmc_areas(price_15000_prediction_interaction) +
  ggplot2::scale_y_discrete(labels = c("std", "turbo")) + 
  xlab("Price")
```
<br>
<br>

The graph of the predictions looks very similar to the one from the no interaction model. The predicted normalized loss is about 110 for a std aspiration car. The predicted normalized loss for a turbo car is a little smaller, but still greater than 100.

# Comparing the Models

Now we will compare the interaction and no-interaction models using cross validation and ELPD techniques. 

## Cross Validation

We will employ 10-fold cross validation to compare the models 
```{r cahce = TRUE}
set.seed(12345)

# No interaction model
no_interact_cv = prediction_summary_cv(model = no_interaction, data = cars_df, k = 10)

# Interaction model
interaction_cv = prediction_summary_cv(model = interaction_model, data = cars_df, k = 10)

# Show summaries 
no_interact_cv$cv
interaction_cv$cv
```

## Expected Log-Predictive Density (ELPD)
```{r}
# Find the ELPD's for the two models
no_interact_loo = loo(no_interaction)
interaction_loo = loo(interaction_model)

# Print out estimates
no_interact_loo$estimates
interaction_loo$estimates

# Compare the models
loo_compare(no_interact_loo, interaction_loo)
```
## Choosing The Model

From the cross validation results, we have the MAE (median absolute error) for the interaction model is larger than that of the no interaction model. However, when comparing the ELPDs from the `loo` comparison, we find that the interaction model has a higher ELPD, meaning it is better than the no interaction model. 

From the analysis, I will choose the interaction model since all regressors are significant in the model and it matches the data visualization. However, there are some concerns in this model. The higher MAE from the cross validation is one such concern and we should be wary to base predictions off of this model. However, the general trend indicated by the exploratory analysis indicates the second model will be a good fit. The coefficients are significant and it has a higher ELPD than the first model.

# Credits and Honor Pledge

## Credits 
- I found the data set on the UCI Machine Learning Repository. The data set's page on the UCI Machine Learning Repository can be found [here](https://archive.ics.uci.edu/ml/datasets/automobile). 

## Honor Pledge

> "On my honor, I have neither received nor given any unauthorized assistance on this project"  
> - Nilay Tripathi
