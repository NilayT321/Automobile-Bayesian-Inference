---
title: "960:365 -- Final Project"
author: "Nilay Tripathi"
date: "12/8/2022"
output: html_document
---

```{r warning=F, message=F}
# Load necessary packages
library(bayesrules)
library(bayesplot)
library(rstanarm)
library(tidyverse)
library(tidybayes)
library(broom.mixed)
library(gridExtra)
library(janitor)
```

# Importing and Cleaning Data

## Importing the Dataset
```{r}
# Define column names
column_names = c("Symboling", "Normalized_Losses", "Make", "Fuel_Type", "Aspiration", "Num_Doors", "Body_Style", "Drive_Wheels", "Engine_Location", "Wheel_Base", "Length", "Width", "Height", "Curb_Weight", "Engine_Type", "Num_Cylinders", "Engine_Size", "Fuel_System", "Bore", "Stroke", "Compression", "Horsepower", "Peak_RPM", "City_MPG", "Highway_MPG", "Price")

# Read the data
cars_data = read_csv("imports-85.data", col_names = column_names)

# Take a peek at the data
head(cars_data)
```

## Data Cleaning

The data in its current form is very messy. We will now clean the data
```{r}
# The values with a '?' for normalized losses are unknown. Remove them from the data set
cars_df = cars_data %>% 
  filter(Normalized_Losses != "?")
head(cars_df)
# Compare number of observations left
c(nrow(cars_data), nrow(cars_df))
# Overall summary for the different columns
summary(cars_df)
# See number of different makes
cars_df %>%
  janitor::tabyl(Make)
# Make a new column called Average_MPG that is the arithmetic mean of the city and highway MPG
cars_df = cars_df %>%
  mutate(Avg_MPG = (City_MPG + Highway_MPG) / 2)
summary(cars_df$Avg_MPG)
# Some of the numeric values have character values. We should change this 
cars_df = cars_df %>%
  mutate(across(.cols = c(2, 19,20,21,22,23,24,26), as.numeric))
# Summary 
summary(cars_df)
# Extra NA values have been identified in column 20. Remove them 
cars_df = cars_df %>% 
  filter(!is.na(Stroke))
summary(cars_df)
```

# Building a Model

We will build a model for `Normalized_Losses`. We will utilize the predictors `price` along with the categorical predictor `Aspiration`.

## Initial Plotting

```{r}
ggplot(data = cars_df, aes(x = Price, y = Normalized_Losses, color = Aspiration)) + 
  geom_point() + 
  geom_smooth(method = "lm", se = FALSE)

# Summary of the values of the cars by fuel type 
cars_df %>% 
  group_by(Aspiration) %>%
  summarize(avg = mean(Normalized_Losses), 
            n = n())
```

From the plot, it appears that there is also an interaction effect in the model. We will assess the usefulness of two models: one with interaction and one without interaction. From the graph, we can see that the linear relationship isn't strong for both variables. 

## Fitting the No-Interaction Model
Here we will fit the model without interaction effects. 
```{r cache = T, results=FALSE}
no_interaction = stan_glm(
  Normalized_Losses ~ Price + Aspiration,
  data = cars_df, family = gaussian,
  prior_intercept = normal(120, 5, autoscale = TRUE),
  prior = normal(0, 2.5, autoscale = TRUE),
  prior_aux = exponential(1, autoscale = TRUE),
  chains = 4, iter = 5000*2, seed = 12345
)
```

### Prior Summary

Here is the prior summary 
```{r}
prior_summary(no_interaction)
```

### MCMC Diagnostics

Here are the MCMC diagnostics for this model
```{r}
# Trace plots
mcmc_trace(no_interaction, size = 0.1)

# Density overlays
mcmc_dens_overlay(no_interaction)

# Autocorrelation plots
mcmc_acf(no_interaction)

# Effective Sample Size ratio
neff_ratio(no_interaction)

# Rhat 
rhat(no_interaction)
```

### Numerical Output

Generate the numerical output of the model using the `tidy` function.
```{r}
tidy(no_interaction, effects = c("fixed", "aux"), conf.int = TRUE, conf.level = 0.80)
```

The output indicates that `Price` is significant (although barely since the coefficients are still pretty close to 0). The categorical variable `Aspiration` does not appear to be significant. We will now calculate the probabilities of these coefficients

### Assessing the Regression Coefficients

We will find the probability that the regression coefficient of `Price` is greater than 0 in our posterior prediction. 
```{r}
no_interaction_df = as.data.frame(no_interaction, include = FALSE)
head(no_interaction_df)

# Probability that Price coefficient is greater than 0
no_interaction_df %>% 
  mutate(regGreaterThan0 = (Price > 0)) %>%
  tabyl(regGreaterThan0)

# Probability that Aspirationturbo coefficient is greater than 0
no_interaction_df %>%
  mutate(aspirationGreaterThan0 = (Aspirationturbo > 0)) %>%
  tabyl(aspirationGreaterThan0)
```

## Fitting the Interaction Model

The graph indicates that there could be an interaction between these models. Let's fit an interaction model to see if it is better than the non-interaction one 
```{r cache = T, results = FALSE}
interaction_model = stan_glm(
  Normalized_Losses ~ Aspiration + Price + Aspiration:Price, 
  data = cars_df, family = gaussian, 
  prior_intercept = normal(120, 5, autoscale = TRUE),
  prior = normal(0, 2.5, autoscale = TRUE),
  prior_aux = exponential(1, autoscale = TRUE),
  chains = 4, iter = 5000*2, seed = 12345
)
```

### Prior Summary
```{r}
prior_summary(interaction_model)
```

### MCMC Diagnostics 
```{r}
# Trace plots
mcmc_trace(interaction_model, size = 0.1)

# Density overlays
mcmc_dens_overlay(interaction_model)

# Autocorrelation plots
mcmc_acf(interaction_model)

# Effective sample size ratio
neff_ratio(interaction_model)

# Rhat value
rhat(interaction_model)
```

### Numerical Output

Here is the `tidy` summary for this model.
```{r}
tidy(interaction_model, effects = c("fixed", "aux"), conf.int = TRUE, conf.level = 0.80)
```

### Interpretation of Coefficients

Here are the interpretation of the coefficients from the interaction model 

# Comparing the Models

Now we will compare the interaction and no-interaction models using cross validation and ELPD techniques. 

## Cross Validation

We will employ 10-fold cross validation to compare the models 
```{r cahce = T}
set.seed(12345)

# No interaction model
no_interact_cv = prediction_summary_cv(model = no_interaction, data = cars_df, k = 10)

# Interaction model
interaction_cv = prediction_summary_cv(model = interaction_model, data = cars_df, k = 10)

# Show summaries 
no_interact_cv$cv
interaction_cv$cv
```

## Expected Log-Predictive Density (ELPD)
```{r}
# Find the ELPD's for the two models
no_interact_loo = loo(no_interaction)
interaction_loo = loo(interaction_model)

# Print out estimates
no_interact_loo$estimates
interaction_loo$estimates

# Compare the models
loo_compare(no_interact_loo, interaction_loo)
```

